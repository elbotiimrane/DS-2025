# Install dependencies as needed:
# pip install kagglehub[pandas-datasets]
import kagglehub
from kagglehub import KaggleDatasetAdapter
import os

# Set the path to the file you'd like to load
# The original file path 'toulouse_library_dataset.csv' was not found.
# We need to find the correct file name within the dataset.
# First, let's list the files available in the dataset directory.
# The error message indicated the dataset path is '/kaggle/input/toulouse-public-library-mdiathque-dataset'
dataset_dir = '/kaggle/input/toulouse-public-library-mdiathque-dataset'
print(f"Files in the dataset directory '{dataset_dir}':")
for filename in os.listdir(dataset_dir):
    print(filename)

# Once you identify the correct CSV file, update the file_path variable below.
file_path = "toulouse_public_library_loans.csv" # Updated with the correct CSV file name from the output above

# Load the latest version
# This part of the code will run after the correct file_path is set.
if file_path:
  df = kagglehub.load_dataset(
    KaggleDatasetAdapter.PANDAS,
    "grimespoint/toulouse-public-library-mdiathque-dataset",
    file_path,
    pandas_kwargs={'engine': 'python', 'on_bad_lines': 'skip', 'sep': ';'}
    # Provide any additional arguments like
    # sql_query or pandas_kwargs. See the
    # documenation for more information:k
    # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas
  )

  print("First 5 records:", df.head())
else:
  print("Please update 'file_path' with the correct CSV file name before loading the dataset.")
     
Files in the dataset directory '/kaggle/input/toulouse-public-library-mdiathque-dataset':
toulouse_public_library_loans.csv
Using Colab cache for faster access to the 'toulouse-public-library-mdiathque-dataset' dataset.
First 5 records:      year  nb_loans                         title                 author  \
0  2023.0       207            Top Gun : Maverick       Kosinski, Joseph   
1  2023.0       193  Trois mille ans à t'attendre         Miller, George   
2  2023.0       152        Compétition officielle          Cohn, Mariano   
3  2023.0       151                Licorice pizza  Anderson, Paul Thomas   
4  2023.0       144                   Le discours        Tirard, Laurent   

                                         publisher classification  library  \
0  Boulogne-Billancourt : Paramount Pictures, 2022        AV TOPG  CABANIS   
1            Paris : Metropolitan filmexport, 2023         F TROI  CABANIS   
2                    Paris : Wild side video, 2022           COMP  CABANIS   
3           Paris : Universal Pictures Vidéo, 2022           LICO  CABANIS   
4                           Paris : Le Pacte, 2021         C DISC  CABANIS   

  spine_label audience media_subtype media_type  
0     AV TOPG        A        DVDFIC      films  
1      F TROI        A        DVDFIC      films  
2        COMP        A        DVDFIC      films  
3        LICO        A        DVDFIC      films  
4      C DISC        A        DVDFIC      films  

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#For ignoring warning
import warnings
warnings.filterwarnings("ignore")
     

# Lung Cancer Prediction Using 10 Machine Learning Models

# ====================================================================
# 1. IMPORTATION DES BIBLIOTHÈQUES
# ====================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score
from imblearn.over_sampling import ADASYN

# ====================================================================
# 2. CHARGEMENT ET EXPLORATION DES DONNÉES
# ====================================================================

# Charger les données
df = pd.read_csv('/kaggle/input/lung-cancer/survey lung cancer.csv')
print("Aperçu des données:")
print(df.head())

print("\nDimensions du dataset:")
print(df.shape)

print("\nVérification des doublons:")
print(df.duplicated().sum())

# Suppression des doublons
df = df.drop_duplicates()

print("\nVérification des valeurs nulles:")
print(df.isnull().sum())

print("\nInformations sur le dataset:")
print(df.info())

print("\nStatistiques descriptives:")
print(df.describe())

# ====================================================================
# 3. ENCODAGE DES VARIABLES
# ====================================================================

# Utiliser LabelEncoder pour convertir les variables catégorielles
le = preprocessing.LabelEncoder()

df['GENDER'] = le.fit_transform(df['GENDER'])
df['LUNG_CANCER'] = le.fit_transform(df['LUNG_CANCER'])
df['SMOKING'] = le.fit_transform(df['SMOKING'])
df['YELLOW_FINGERS'] = le.fit_transform(df['YELLOW_FINGERS'])
df['ANXIETY'] = le.fit_transform(df['ANXIETY'])
df['PEER_PRESSURE'] = le.fit_transform(df['PEER_PRESSURE'])
df['CHRONIC DISEASE'] = le.fit_transform(df['CHRONIC DISEASE'])
df['FATIGUE '] = le.fit_transform(df['FATIGUE '])
df['ALLERGY '] = le.fit_transform(df['ALLERGY '])
df['WHEEZING'] = le.fit_transform(df['WHEEZING'])
df['ALCOHOL CONSUMING'] = le.fit_transform(df['ALCOHOL CONSUMING'])
df['COUGHING'] = le.fit_transform(df['COUGHING'])
df['SHORTNESS OF BREATH'] = le.fit_transform(df['SHORTNESS OF BREATH'])
df['SWALLOWING DIFFICULTY'] = le.fit_transform(df['SWALLOWING DIFFICULTY'])
df['CHEST PAIN'] = le.fit_transform(df['CHEST PAIN'])

print("\nDonnées après encodage:")
print(df.head())

# ====================================================================
# 4. VISUALISATION DE LA DISTRIBUTION DE LA CIBLE
# ====================================================================

plt.figure(figsize=(8, 5))
sns.countplot(x='LUNG_CANCER', data=df)
plt.title('Distribution de la variable cible')
plt.show()

print("\nDistribution de LUNG_CANCER:")
print(df['LUNG_CANCER'].value_counts())

# ====================================================================
# 5. VISUALISATION DES RELATIONS AVEC LA CIBLE
# ====================================================================

def plot(col, df=df):
    return df.groupby(col)['LUNG_CANCER'].value_counts(normalize=True).unstack().plot(kind='bar', figsize=(8,5))

# Visualiser chaque variable
for col in ['GENDER', 'AGE', 'SMOKING', 'YELLOW_FINGERS', 'ANXIETY',
            'PEER_PRESSURE', 'CHRONIC DISEASE', 'FATIGUE ', 'ALLERGY ',
            'WHEEZING', 'ALCOHOL CONSUMING', 'COUGHING', 'SHORTNESS OF BREATH',
            'SWALLOWING DIFFICULTY', 'CHEST PAIN']:
    plt.figure()
    plot(col)
    plt.title(f'Relation entre {col} et LUNG_CANCER')
    plt.show()

# ====================================================================
# 6. SUPPRESSION DES VARIABLES PEU CORRÉLÉES
# ====================================================================

df_new = df.drop(columns=['GENDER', 'AGE', 'SMOKING', 'SHORTNESS OF BREATH'])
print("\nNouveau dataset après suppression de colonnes:")
print(df_new.head())

# ====================================================================
# 7. ANALYSE DE CORRÉLATION
# ====================================================================

cn = df_new.corr()
print("\nMatrice de corrélation:")
print(cn)

# Heatmap de corrélation
plt.figure(figsize=(18, 18))
cmap = sns.diverging_palette(260, -10, s=50, l=75, n=6, as_cmap=True)
sns.heatmap(cn, cmap=cmap, annot=True, square=True)
plt.title('Matrice de corrélation')
plt.show()

# Heatmap des corrélations >= 0.40
plt.figure(figsize=(12, 8))
kot = cn[cn >= 0.40]
sns.heatmap(kot, cmap="Blues")
plt.title('Corrélations >= 0.40')
plt.show()

# ====================================================================
# 8. FEATURE ENGINEERING
# ====================================================================

# Créer une nouvelle variable combinant ANXIETY et YELLOW_FINGERS
df_new['ANXYELFIN'] = df_new['ANXIETY'] * df_new['YELLOW_FINGERS']
print("\nDataset avec nouvelle variable ANXYELFIN:")
print(df_new.head())

# ====================================================================
# 9. SÉPARATION DES VARIABLES
# ====================================================================

X = df_new.drop('LUNG_CANCER', axis=1)
y = df_new['LUNG_CANCER']

# ====================================================================
# 10. GESTION DU DÉSÉQUILIBRE DES CLASSES (ADASYN)
# ====================================================================

adasyn = ADASYN(random_state=42)
X, y = adasyn.fit_resample(X, y)
print(f"\nNombre d'échantillons après ADASYN: {len(X)}")

# ====================================================================
# 11. DIVISION DES DONNÉES (TRAIN/TEST)
# ====================================================================

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# ====================================================================
# 12. ENTRAÎNEMENT DES MODÈLES
# ====================================================================

print("\n" + "="*70)
print("ENTRAÎNEMENT ET ÉVALUATION DES MODÈLES")
print("="*70)

# 12.1 Logistic Regression
print("\n1. LOGISTIC REGRESSION")
lr_model = LogisticRegression(random_state=0)
lr_model.fit(X_train, y_train)
y_lr_pred = lr_model.predict(X_test)
print(classification_report(y_test, y_lr_pred))

# 12.2 Decision Tree
print("\n2. DECISION TREE")
dt_model = DecisionTreeClassifier(criterion='entropy', random_state=0)
dt_model.fit(X_train, y_train)
y_dt_pred = dt_model.predict(X_test)
print(classification_report(y_test, y_dt_pred))

# 12.3 K-Nearest Neighbors
print("\n3. K-NEAREST NEIGHBORS")
knn_model = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
knn_model.fit(X_train, y_train)
y_knn_pred = knn_model.predict(X_test)
print(classification_report(y_test, y_knn_pred))

# 12.4 Gaussian Naive Bayes
print("\n4. GAUSSIAN NAIVE BAYES")
gnb_model = GaussianNB()
gnb_model.fit(X_train, y_train)
y_gnb_pred = gnb_model.predict(X_test)
print(classification_report(y_test, y_gnb_pred))

# 12.5 Multinomial Naive Bayes
print("\n5. MULTINOMIAL NAIVE BAYES")
mnb_model = MultinomialNB()
mnb_model.fit(X_train, y_train)
y_mnb_pred = mnb_model.predict(X_test)
print(classification_report(y_test, y_mnb_pred))

# 12.6 Support Vector Classifier
print("\n6. SUPPORT VECTOR CLASSIFIER")
svc_model = SVC()
svc_model.fit(X_train, y_train)
y_svc_pred = svc_model.predict(X_test)
print(classification_report(y_test, y_svc_pred))

# 12.7 Random Forest
print("\n7. RANDOM FOREST")
rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
y_rf_pred = rf_model.predict(X_test)
print(classification_report(y_test, y_rf_pred))

# 12.8 XGBoost
print("\n8. XGBOOST")
xgb_model = XGBClassifier()
xgb_model.fit(X_train, y_train)
y_xgb_pred = xgb_model.predict(X_test)
print(classification_report(y_test, y_xgb_pred))

# 12.9 Multi-layer Perceptron
print("\n9. MULTI-LAYER PERCEPTRON")
mlp_model = MLPClassifier()
mlp_model.fit(X_train, y_train)
y_mlp_pred = mlp_model.predict(X_test)
print(classification_report(y_test, y_mlp_pred))

# 12.10 Gradient Boosting
print("\n10. GRADIENT BOOSTING")
gb_model = GradientBoostingClassifier()
gb_model.fit(X_train, y_train)
y_gb_pred = gb_model.predict(X_test)
print(classification_report(y_test, y_gb_pred))

# ====================================================================
# 13. VALIDATION CROISÉE - K-FOLD
# ====================================================================

print("\n" + "="*70)
print("VALIDATION CROISÉE - K-FOLD (k=10)")
print("="*70)

k = 10
kf = KFold(n_splits=k, shuffle=True, random_state=42)

lr_scores = cross_val_score(lr_model, X, y, cv=kf)
dt_scores = cross_val_score(dt_model, X, y, cv=kf)
knn_scores = cross_val_score(knn_model, X, y, cv=kf)
gnb_scores = cross_val_score(gnb_model, X, y, cv=kf)
mnb_scores = cross_val_score(mnb_model, X, y, cv=kf)
svc_scores = cross_val_score(svc_model, X, y, cv=kf)
rf_scores = cross_val_score(rf_model, X, y, cv=kf)
xgb_scores = cross_val_score(xgb_model, X, y, cv=kf)
mlp_scores = cross_val_score(mlp_model, X, y, cv=kf)
gb_scores = cross_val_score(gb_model, X, y, cv=kf)

print(f"\nLogistic Regression - Précision moyenne: {np.mean(lr_scores):.4f}")
print(f"Decision Tree - Précision moyenne: {np.mean(dt_scores):.4f}")
print(f"KNN - Précision moyenne: {np.mean(knn_scores):.4f}")
print(f"Gaussian Naive Bayes - Précision moyenne: {np.mean(gnb_scores):.4f}")
print(f"Multinomial Naive Bayes - Précision moyenne: {np.mean(mnb_scores):.4f}")
print(f"Support Vector Classifier - Précision moyenne: {np.mean(svc_scores):.4f}")
print(f"Random Forest - Précision moyenne: {np.mean(rf_scores):.4f}")
print(f"XGBoost - Précision moyenne: {np.mean(xgb_scores):.4f}")
print(f"Multi-layer Perceptron - Précision moyenne: {np.mean(mlp_scores):.4f}")
print(f"Gradient Boosting - Précision moyenne: {np.mean(gb_scores):.4f}")

# ====================================================================
# 14. VALIDATION CROISÉE - STRATIFIED K-FOLD
# ====================================================================

print("\n" + "="*70)
print("VALIDATION CROISÉE - STRATIFIED K-FOLD (k=10)")
print("="*70)

skf = StratifiedKFold(n_splits=k)

lr_scores_s = cross_val_score(lr_model, X, y, cv=skf)
dt_scores_s = cross_val_score(dt_model, X, y, cv=skf)
knn_scores_s = cross_val_score(knn_model, X, y, cv=skf)
gnb_scores_s = cross_val_score(gnb_model, X, y, cv=skf)
mnb_scores_s = cross_val_score(mnb_model, X, y, cv=skf)
svc_scores_s = cross_val_score(svc_model, X, y, cv=skf)
rf_scores_s = cross_val_score(rf_model, X, y, cv=skf)
xgb_scores_s = cross_val_score(xgb_model, X, y, cv=skf)
mlp_scores_s = cross_val_score(mlp_model, X, y, cv=skf)
gb_scores_s = cross_val_score(gb_model, X, y, cv=skf)

print(f"\nLogistic Regression - Précision moyenne: {np.mean(lr_scores_s):.4f}")
print(f"Decision Tree - Précision moyenne: {np.mean(dt_scores_s):.4f}")
print(f"KNN - Précision moyenne: {np.mean(knn_scores_s):.4f}")
print(f"Gaussian Naive Bayes - Précision moyenne: {np.mean(gnb_scores_s):.4f}")
print(f"Multinomial Naive Bayes - Précision moyenne: {np.mean(mnb_scores_s):.4f}")
print(f"Support Vector Classifier - Précision moyenne: {np.mean(svc_scores_s):.4f}")
print(f"Random Forest - Précision moyenne: {np.mean(rf_scores_s):.4f}")
print(f"XGBoost - Précision moyenne: {np.mean(xgb_scores_s):.4f}")
print(f"Multi-layer Perceptron - Précision moyenne: {np.mean(mlp_scores_s):.4f}")
print(f"Gradient Boosting - Précision moyenne: {np.mean(gb_scores_s):.4f}")

print("\n" + "="*70)
print("FIN DE L'ANALYSE")
print("="*70)
     
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
/tmp/ipython-input-239902031.py in <cell line: 0>()
     30 
     31 # Charger les données
---> 32 df = pd.read_csv('/kaggle/input/lung-cancer/survey lung cancer.csv')
     33 print("Aperçu des données:")
     34 print(df.head())

/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)
   1024     kwds.update(kwds_defaults)
   1025 
-> 1026     return _read(filepath_or_buffer, kwds)
   1027 
   1028 

/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py in _read(filepath_or_buffer, kwds)
    618 
    619     # Create the parser.
--> 620     parser = TextFileReader(filepath_or_buffer, **kwds)
    621 
    622     if chunksize or iterator:

/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py in __init__(self, f, engine, **kwds)
   1618 
   1619         self.handles: IOHandles | None = None
-> 1620         self._engine = self._make_engine(f, self.engine)
   1621 
   1622     def close(self) -> None:

/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py in _make_engine(self, f, engine)
   1878                 if "b" not in mode:
   1879                     mode += "b"
-> 1880             self.handles = get_handle(
   1881                 f,
   1882                 mode,

/usr/local/lib/python3.12/dist-packages/pandas/io/common.py in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)
    871         if ioargs.encoding and "b" not in ioargs.mode:
    872             # Encoding
--> 873             handle = open(
    874                 handle,
    875                 ioargs.mode,

FileNotFoundError: [Errno 2] No such file or directory: '/kaggle/input/lung-cancer/survey lung cancer.csv'
